{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2eca48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4877\n",
      "Epoch 2, Loss: 1.4714\n",
      "Epoch 3, Loss: 1.4554\n",
      "Epoch 4, Loss: 1.4396\n",
      "Epoch 5, Loss: 1.4240\n",
      "Epoch 6, Loss: 1.4088\n",
      "Epoch 7, Loss: 1.3937\n",
      "Epoch 8, Loss: 1.3790\n",
      "Epoch 9, Loss: 1.3644\n",
      "Epoch 10, Loss: 1.3500\n",
      "Epoch 11, Loss: 1.3359\n",
      "Epoch 12, Loss: 1.3219\n",
      "Epoch 13, Loss: 1.3082\n",
      "Epoch 14, Loss: 1.2948\n",
      "Epoch 15, Loss: 1.2815\n",
      "Epoch 16, Loss: 1.2684\n",
      "Epoch 17, Loss: 1.2555\n",
      "Epoch 18, Loss: 1.2428\n",
      "Epoch 19, Loss: 1.2303\n",
      "Epoch 20, Loss: 1.2180\n",
      "\n",
      "Model and encoders saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary (1).csv\")\n",
    "\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "features = [\n",
    "    'vehicle_type_enc', 'most_frequent_lane', 'entry_enc',\n",
    "    'duration', 'idle_time', 'co2_emission_g', 'direction_enc'\n",
    "]\n",
    "target = 'exit_enc'\n",
    "\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data[target].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "class ExitPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ExitPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = ExitPredictor(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"exit_path_model.pt\")\n",
    "import joblib\n",
    "joblib.dump((scaler, le_vehicle_type, le_direction, le_entry, le_exit), \"encoders.pkl\")\n",
    "print(\"\\nModel and encoders saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2d97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed directions: ['0->2' '2->1' '2->3' '3->0' '3->2']\n",
      "Allowed entries: [0 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Allowed directions:\", le_direction.classes_)\n",
    "print(\"Allowed entries:\", le_entry.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e047d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted optimal exit zone: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ExitPredictor(input_size=7, hidden_size=64, num_classes=len(le_exit.classes_))\n",
    "model.load_state_dict(torch.load(\"exit_path_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "scaler, le_vehicle_type, le_direction, le_entry, le_exit = joblib.load(\"encoders.pkl\")\n",
    "\n",
    "def predict_exit_zone(model, scaler, le_vehicle_type, le_direction, le_entry, le_exit,\n",
    "                      vehicle_type, most_frequent_lane, entry_zone_id,\n",
    "                      duration, idle_time, co2_emission_g, direction):\n",
    "    if isinstance(entry_zone_id, str) and entry_zone_id.startswith('E'):\n",
    "        entry_zone_id = entry_zone_id[1:]\n",
    "\n",
    "    vehicle_type_enc = le_vehicle_type.transform([vehicle_type])[0]\n",
    "    direction_enc = le_direction.transform([direction])[0]\n",
    "    entry_enc = le_entry.transform([entry_zone_id])[0]\n",
    "\n",
    "    input_vector = np.array([[vehicle_type_enc, most_frequent_lane, entry_enc,\n",
    "                              duration, idle_time, co2_emission_g, direction_enc]])\n",
    "    input_scaled = scaler.transform(input_vector)\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "        predicted_label = le_exit.inverse_transform([predicted_idx])[0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Call prediction\n",
    "predicted = predict_exit_zone(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    le_vehicle_type=le_vehicle_type,\n",
    "    le_direction=le_direction,\n",
    "    le_entry=le_entry,\n",
    "    le_exit=le_exit,\n",
    "    vehicle_type=\"car\",\n",
    "    most_frequent_lane=2,\n",
    "    entry_zone_id=\"E2\",    \n",
    "    duration=15.5,\n",
    "    idle_time=3.0,\n",
    "    co2_emission_g=280.0,\n",
    "    direction=\"2->3\"\n",
    ")\n",
    "\n",
    "print(\"Predicted optimal exit zone:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c7bffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      8\u001b[0m     true_exit \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_exit_zone\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m     pred_exit \u001b[38;5;241m=\u001b[39m predict_exit_zone(\n\u001b[0;32m     10\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     11\u001b[0m         scaler\u001b[38;5;241m=\u001b[39mscaler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         direction\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    true_exit = row['true_exit_zone']\n",
    "    pred_exit = predict_exit_zone(\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        le_vehicle_type=le_vehicle_type,\n",
    "        le_direction=le_direction,\n",
    "        le_entry=le_entry,\n",
    "        le_exit=le_exit,\n",
    "        vehicle_type=row['vehicle_type'],\n",
    "        most_frequent_lane=row['most_frequent_lane'],\n",
    "        entry_zone_id=row['entry_zone_id'],\n",
    "        duration=row['duration'],\n",
    "        idle_time=row['idle_time'],\n",
    "        co2_emission_g=row['co2_emission_g'],\n",
    "        direction=row['direction']\n",
    "    )\n",
    "    \n",
    "    true_labels.append(true_exit)\n",
    "    predicted_labels.append(pred_exit)\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1dd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lane-wise traffic time series to 'lane_traffic_time_series.csv'\n",
      "Interactive traffic density graph saved as 'traffic_density_plot.html'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Load your vehicle summary CSV\n",
    "df = pd.read_csv(\"vehicle_summary (1).csv\")\n",
    "\n",
    "# Time range: second-by-second\n",
    "min_time = int(df['start_time'].min())\n",
    "max_time = int(df['end_time'].max())\n",
    "time_range = range(min_time, max_time + 1)\n",
    "\n",
    "# Prepare traffic counts per lane\n",
    "lane_ids = sorted(df['most_frequent_lane'].unique())\n",
    "lane_traffic = {lane: [] for lane in lane_ids}\n",
    "\n",
    "for t in time_range:\n",
    "    active = df[(df['start_time'] <= t) & (df['end_time'] >= t)]\n",
    "    for lane in lane_ids:\n",
    "        lane_traffic[lane].append((active['most_frequent_lane'] == lane).sum())\n",
    "\n",
    "# Convert to DataFrame\n",
    "traffic_df = pd.DataFrame(lane_traffic, index=time_range)\n",
    "traffic_df.index.name = \"time\"\n",
    "traffic_df.to_csv(\"lane_traffic_time_series.csv\")\n",
    "print(\"Saved lane-wise traffic time series to 'lane_traffic_time_series.csv'\")\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for lane in lane_ids:\n",
    "    fig.add_trace(go.Scatter(x=traffic_df.index, y=traffic_df[lane],\n",
    "                             mode='lines',\n",
    "                             name=f\"Lane {lane}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Traffic Density Over Time (Per Lane)\",\n",
    "    xaxis_title=\"Time (s)\",\n",
    "    yaxis_title=\"Number of Vehicles\",\n",
    "    legend_title=\"Lanes\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Save the figure to HTML\n",
    "pio.write_html(fig, file=\"traffic_density_plot.html\", auto_open=True)\n",
    "print(\"Interactive traffic density graph saved as 'traffic_density_plot.html'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434f9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stacked green signal optimization chart to green_light_optimization.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df = pd.read_csv(\"lane_traffic_time_series.csv\", index_col='time')\n",
    "\n",
    "df['slot'] = (df.index // 1800) * 1800  # 1800 sec = 30 min\n",
    "df['slot'] = pd.to_datetime(df['slot'], unit='s').dt.strftime('%H:%M')\n",
    "\n",
    "slot_summary = df.groupby('slot').sum()\n",
    "\n",
    "cycle_time = 120\n",
    "\n",
    "green_times = slot_summary.div(slot_summary.sum(axis=1), axis=0) * cycle_time\n",
    "green_times = green_times.round(1)\n",
    "\n",
    "fig = go.Figure()\n",
    "colors = ['#f5c71a', '#1f77b4', '#7f7f7f', '#d95f02']  # Matching colors\n",
    "\n",
    "for i, lane in enumerate(green_times.columns):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=f\"Green Signal for Path {lane}\",\n",
    "        x=green_times.index,\n",
    "        y=green_times[lane],\n",
    "        marker_color=colors[i % len(colors)],\n",
    "        text=green_times[lane],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title=\"Green Light Optimization for all four paths\",\n",
    "    xaxis_title=\"Time Slot\",\n",
    "    yaxis_title=\"Green Time (seconds)\",\n",
    "    template=\"plotly_white\",\n",
    "    legend_title=\"Paths\"\n",
    ")\n",
    "\n",
    "fig.write_html(\"green_light_optimization.html\", auto_open=True)\n",
    "print(\"Saved stacked green signal optimization chart to green_light_optimization.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e883b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lane-wise traffic time series to 'lane_traffic_time_series.csv'\n",
      "Interactive traffic density graph saved as 'traffic_density_plot.html'\n",
      "Interactive green signal optimization graph saved as 'green_signal_optimization_plot.html'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "# Load your vehicle summary CSV\n",
    "df = pd.read_csv(\"vehicle_summary (1).csv\")\n",
    "\n",
    "# Time range: second-by-second\n",
    "min_time = int(df['start_time'].min())\n",
    "max_time = int(df['end_time'].max())\n",
    "time_range = range(min_time, max_time + 1)\n",
    "\n",
    "# Prepare traffic counts per lane\n",
    "lane_ids = sorted(df['most_frequent_lane'].unique())\n",
    "lane_traffic = {lane: [] for lane in lane_ids}\n",
    "\n",
    "for t in time_range:\n",
    "    active = df[(df['start_time'] <= t) & (df['end_time'] >= t)]\n",
    "    for lane in lane_ids:\n",
    "        lane_traffic[lane].append((active['most_frequent_lane'] == lane).sum())\n",
    "\n",
    "# Convert to DataFrame\n",
    "traffic_df = pd.DataFrame(lane_traffic, index=time_range)\n",
    "traffic_df.index.name = \"time\"\n",
    "traffic_df.to_csv(\"lane_traffic_time_series.csv\")\n",
    "print(\"Saved lane-wise traffic time series to 'lane_traffic_time_series.csv'\")\n",
    "\n",
    "# Plot traffic density over time\n",
    "fig = go.Figure()\n",
    "for lane in lane_ids:\n",
    "    fig.add_trace(go.Scatter(x=traffic_df.index, y=traffic_df[lane],\n",
    "                             mode='lines',\n",
    "                             name=f\"Lane {lane}\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Traffic Density Over Time (Per Lane)\",\n",
    "    xaxis_title=\"Time (s)\",\n",
    "    yaxis_title=\"Number of Vehicles\",\n",
    "    legend_title=\"Lanes\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "pio.write_html(fig, file=\"traffic_density_plot.html\", auto_open=True)\n",
    "print(\"Interactive traffic density graph saved as 'traffic_density_plot.html'\")\n",
    "\n",
    "# -------------------------- Green Light Optimization --------------------------\n",
    "interval_size = 5  # 5-second intervals\n",
    "total_cycle_time = 120  # Total green light cycle time in seconds\n",
    "service_rate = 20  # Œº in vehicles/sec\n",
    "\n",
    "intervals = list(range(min_time, max_time + 1, interval_size))\n",
    "green_time_df = pd.DataFrame(columns=['interval_start', 'interval_end'] + [f'lane_{lane}_g' for lane in lane_ids])\n",
    "\n",
    "for i in range(len(intervals) - 1):\n",
    "    start, end = intervals[i], intervals[i + 1]\n",
    "    sub_df = df[(df['start_time'] >= start) & (df['start_time'] < end)]\n",
    "    arrival_rates = []\n",
    "    for lane in lane_ids:\n",
    "        arrivals = (sub_df['most_frequent_lane'] == lane).sum()\n",
    "        Œª = arrivals / interval_size  # vehicles/sec\n",
    "        arrival_rates.append(Œª)\n",
    "\n",
    "    Œª_total = sum(arrival_rates) if sum(arrival_rates) > 0 else 1\n",
    "    green_times = [(Œª / Œª_total) * total_cycle_time for Œª in arrival_rates]\n",
    "\n",
    "    green_time_df.loc[i] = [start, end] + green_times\n",
    "\n",
    "# Plot optimized green time for each lane\n",
    "fig2 = go.Figure()\n",
    "for idx, lane in enumerate(lane_ids):\n",
    "    fig2.add_trace(go.Scatter(\n",
    "        x=green_time_df['interval_start'],\n",
    "        y=green_time_df[f'lane_{lane}_g'],\n",
    "        mode='lines+markers',\n",
    "        name=f\"Lane {lane}\"\n",
    "    ))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"Green Signal Duration Over Time (Per Lane)\",\n",
    "    xaxis_title=\"Time Interval Start (s)\",\n",
    "    yaxis_title=\"Green Time (s)\",\n",
    "    legend_title=\"Lanes\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "pio.write_html(fig2, file=\"green_signal_optimization_plot.html\", auto_open=True)\n",
    "print(\"Interactive green signal optimization graph saved as 'green_signal_optimization_plot.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca52d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3529\n",
      "Epoch 2, Loss: 1.3588\n",
      "Epoch 3, Loss: 1.3424\n",
      "Epoch 4, Loss: 1.3433\n",
      "Epoch 5, Loss: 1.2718\n",
      "Epoch 6, Loss: 1.2243\n",
      "Epoch 7, Loss: 1.2507\n",
      "Epoch 8, Loss: 1.1697\n",
      "Epoch 9, Loss: 1.1977\n",
      "Epoch 10, Loss: 1.1358\n",
      "Epoch 11, Loss: 1.1438\n",
      "Epoch 12, Loss: 1.1416\n",
      "Epoch 13, Loss: 1.1209\n",
      "Epoch 14, Loss: 1.0686\n",
      "Epoch 15, Loss: 1.0855\n",
      "Epoch 16, Loss: 1.1302\n",
      "Epoch 17, Loss: 1.1399\n",
      "Epoch 18, Loss: 1.0154\n",
      "Epoch 19, Loss: 1.1215\n",
      "Epoch 20, Loss: 1.0137\n",
      "\n",
      "Model Accuracy: 0.5714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.62      1.00      0.77         5\n",
      "           3       0.50      0.60      0.55         5\n",
      "\n",
      "    accuracy                           0.57        14\n",
      "   macro avg       0.28      0.40      0.33        14\n",
      "weighted avg       0.40      0.57      0.47        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load your file\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary_with_50_extra_rows.csv\")\n",
    "\n",
    "# Encode categorical features\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "# Features & target\n",
    "features = [\n",
    "    'vehicle_type_enc', 'most_frequent_lane', 'entry_enc',\n",
    "    'duration', 'idle_time', 'co2_emission_g', 'direction_enc'\n",
    "]\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data['exit_enc'].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class ExitPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ExitPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Initialize\n",
    "input_size = len(features)\n",
    "hidden_size = 64\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = ExitPredictor(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1).numpy()\n",
    "\n",
    "# Report\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "report = classification_report(y_test, predicted_classes, target_names=le_exit.classes_.astype(str))\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c694fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'artist' from 'matplotlib' (C:\\Python310\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create confusion matrix\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\matplotlib\\colorbar.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, collections, cm, colors, contour, ticker\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\matplotlib\\collections.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, cm, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, docstring,\n\u001b[0;32m     21\u001b[0m                hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\matplotlib\\lines.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, artist, cbook, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, docstring, rcParams\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artist, allow_rasterization\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     _to_unmasked_float_array, ls_mapper, ls_mapper_r, STEP_LOOKUP_MAP)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'artist' from 'matplotlib' (C:\\Python310\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "labels = le_exit.classes_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix - Exit Zone Prediction\")\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "# Annotate cells\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('Actual Exit Zone')\n",
    "plt.xlabel('Predicted Exit Zone')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cc5f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4419\n",
      "Epoch 2, Loss: 1.4374\n",
      "Epoch 3, Loss: 1.5236\n",
      "Epoch 4, Loss: 1.2685\n",
      "Epoch 5, Loss: 1.3230\n",
      "Epoch 6, Loss: 1.1896\n",
      "Epoch 7, Loss: 1.3598\n",
      "Epoch 8, Loss: 1.2471\n",
      "Epoch 9, Loss: 1.2066\n",
      "Epoch 10, Loss: 1.1415\n",
      "Epoch 11, Loss: 0.9597\n",
      "Epoch 12, Loss: 1.3169\n",
      "Epoch 13, Loss: 1.0733\n",
      "Epoch 14, Loss: 1.2306\n",
      "Epoch 15, Loss: 1.0463\n",
      "Epoch 16, Loss: 1.0169\n",
      "Epoch 17, Loss: 1.1771\n",
      "Epoch 18, Loss: 1.0716\n",
      "Epoch 19, Loss: 1.1876\n",
      "Epoch 20, Loss: 1.1579\n",
      "Epoch 21, Loss: 1.0406\n",
      "Epoch 22, Loss: 1.1529\n",
      "Epoch 23, Loss: 0.7845\n",
      "Epoch 24, Loss: 1.0101\n",
      "Epoch 25, Loss: 0.9043\n",
      "Epoch 26, Loss: 1.0010\n",
      "Epoch 27, Loss: 0.8604\n",
      "Epoch 28, Loss: 0.9996\n",
      "Epoch 29, Loss: 0.7530\n",
      "Epoch 30, Loss: 0.9733\n",
      "Epoch 31, Loss: 0.7831\n",
      "Epoch 32, Loss: 0.8513\n",
      "Epoch 33, Loss: 0.8146\n",
      "Epoch 34, Loss: 0.9593\n",
      "Epoch 35, Loss: 0.8117\n",
      "Epoch 36, Loss: 0.7403\n",
      "Epoch 37, Loss: 0.8481\n",
      "Epoch 38, Loss: 0.7014\n",
      "Epoch 39, Loss: 0.9865\n",
      "Epoch 40, Loss: 0.8867\n",
      "Epoch 41, Loss: 0.5135\n",
      "Epoch 42, Loss: 0.5693\n",
      "Epoch 43, Loss: 0.7425\n",
      "Epoch 44, Loss: 0.7283\n",
      "Epoch 45, Loss: 0.8566\n",
      "Epoch 46, Loss: 0.6587\n",
      "Epoch 47, Loss: 0.7000\n",
      "Epoch 48, Loss: 0.6671\n",
      "Epoch 49, Loss: 0.6860\n",
      "Epoch 50, Loss: 0.5926\n",
      "\n",
      "‚úÖ Model Accuracy: 0.7143\n",
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       0.62      1.00      0.77         5\n",
      "           3       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.71        14\n",
      "   macro avg       0.61      0.57      0.56        14\n",
      "weighted avg       0.65      0.71      0.66        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary_with_50_extra_rows.csv\")\n",
    "\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "features = [\n",
    "    'entry_enc', 'direction_enc', 'most_frequent_lane', 'duration', 'vehicle_type_enc'\n",
    "]\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data['exit_enc'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=16, shuffle=True)\n",
    "\n",
    "class ExitPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ExitPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_size = len(features)\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = ExitPredictor(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1).numpy()\n",
    "\n",
    "# Evaluation report\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "report = classification_report(y_test, predicted_classes, target_names=le_exit.classes_.astype(str))\n",
    "\n",
    "print(f\"\\n‚úÖ Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nüìä Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8d0b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Tuning and Training: Random Forest\n",
      "\n",
      "üîç Tuning and Training: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Tuning and Training: Gradient Boosting\n",
      "\n",
      "üîç Tuning and Training: Logistic Regression\n",
      "\n",
      "üîç Tuning and Training: MLP Classifier\n",
      "\n",
      "üìà Model Comparison Results:\n",
      "\n",
      "=== Random Forest ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'max_depth': None, 'n_estimators': 50}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n",
      "=== XGBoost ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n",
      "=== Gradient Boosting ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n",
      "=== Logistic Regression ===\n",
      "‚úÖ Accuracy: 0.7143\n",
      "üõ†Ô∏è Best Params: {'C': 10, 'solver': 'liblinear'}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.60      0.60      0.60         5\n",
      "           3       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.71        14\n",
      "   macro avg       0.80      0.80      0.80        14\n",
      "weighted avg       0.71      0.71      0.71        14\n",
      "\n",
      "========================================\n",
      "=== MLP Classifier ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Data\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary_with_50_extra_rows.csv\")\n",
    "\n",
    "# Encoding categorical features\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "features = ['entry_enc', 'direction_enc', 'most_frequent_lane', 'duration', 'vehicle_type_enc']\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data['exit_enc'].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models with grid search params\n",
    "models = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 150],\n",
    "            \"max_depth\": [None, 10, 20]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"solver\": [\"liblinear\", \"lbfgs\"]\n",
    "        }\n",
    "    },\n",
    "    \"MLP Classifier\": {\n",
    "        \"model\": MLPClassifier(max_iter=1000),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(100,), (128, 64)],\n",
    "            \"activation\": ['relu', 'tanh'],\n",
    "            \"alpha\": [0.0001, 0.001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\nüîç Tuning and Training: {name}\")\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    preds = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    report = classification_report(y_test, preds, target_names=le_exit.classes_.astype(str))\n",
    "    \n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"classification_report\": report,\n",
    "        \"best_params\": clf.best_params_\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìà Model Comparison Results:\\n\")\n",
    "for model_name, res in results.items():\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"‚úÖ Accuracy: {res['accuracy']:.4f}\")\n",
    "    print(f\"üõ†Ô∏è Best Params: {res['best_params']}\")\n",
    "    print(f\"üìä Classification Report:\\n{res['classification_report']}\")\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1af8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìâ Realistic Accuracy: 1.0000\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary_with_50_extra_rows.csv\")\n",
    "\n",
    "# Encode categorical variables\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "# üß† CHOOSE FEATURES CAREFULLY TO REDUCE LEAKAGE\n",
    "# Removed 'entry_enc' to avoid direct mapping to 'exit_enc'\n",
    "features = ['direction_enc', 'most_frequent_lane', 'duration', 'vehicle_type_enc']\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data['exit_enc'].values\n",
    "\n",
    "# üé≤ Add Gaussian noise to simulate real-world imperfection\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 0.3, X.shape)\n",
    "X_noisy = X + noise\n",
    "\n",
    "# üîß Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_noisy)\n",
    "\n",
    "# ‚úÇÔ∏è Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# üöÄ Train XGBoost Classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', \n",
    "                      n_estimators=50, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# üß™ Test & Evaluate\n",
    "preds = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "report = classification_report(y_test, preds, target_names=le_exit.classes_.astype(str))\n",
    "\n",
    "# üìä Show results\n",
    "print(f\"\\nüìâ Realistic Accuracy: {acc:.4f}\")\n",
    "print(\"üìä Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168d10cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Tuning and Training: Random Forest\n",
      "\n",
      "üîç Tuning and Training: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\SK VERMA\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Tuning and Training: Gradient Boosting\n",
      "\n",
      "üîç Tuning and Training: Logistic Regression\n",
      "\n",
      "üîç Tuning and Training: MLP Classifier\n",
      "\n",
      "üìà Model Comparison Results:\n",
      "\n",
      "=== Random Forest ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'max_depth': 5, 'n_estimators': 50}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n",
      "=== XGBoost ===\n",
      "‚úÖ Accuracy: 1.0000\n",
      "üõ†Ô∏è Best Params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "========================================\n",
      "=== Gradient Boosting ===\n",
      "‚úÖ Accuracy: 0.9286\n",
      "üõ†Ô∏è Best Params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.96      0.88      0.89        14\n",
      "weighted avg       0.94      0.93      0.92        14\n",
      "\n",
      "========================================\n",
      "=== Logistic Regression ===\n",
      "‚úÖ Accuracy: 0.3571\n",
      "üõ†Ô∏è Best Params: {'C': 1, 'solver': 'lbfgs'}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       0.25      0.20      0.22         5\n",
      "           3       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.36        14\n",
      "   macro avg       0.46      0.32      0.37        14\n",
      "weighted avg       0.45      0.36      0.39        14\n",
      "\n",
      "========================================\n",
      "=== MLP Classifier ===\n",
      "‚úÖ Accuracy: 0.9286\n",
      "üõ†Ô∏è Best Params: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (128,)}\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.67      1.00      0.80         2\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.92      0.95      0.92        14\n",
      "weighted avg       0.95      0.93      0.93        14\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Data\n",
    "vehicle_data = pd.read_csv(\"vehicle_summary_with_50_extra_rows.csv\")\n",
    "\n",
    "# Encoding categorical features\n",
    "le_vehicle_type = LabelEncoder()\n",
    "le_direction = LabelEncoder()\n",
    "le_entry = LabelEncoder()\n",
    "le_exit = LabelEncoder()\n",
    "\n",
    "vehicle_data['vehicle_type_enc'] = le_vehicle_type.fit_transform(vehicle_data['vehicle_type'])\n",
    "vehicle_data['direction_enc'] = le_direction.fit_transform(vehicle_data['direction'])\n",
    "vehicle_data['entry_enc'] = le_entry.fit_transform(vehicle_data['entry_zone_id'])\n",
    "vehicle_data['exit_enc'] = le_exit.fit_transform(vehicle_data['exit_zone_id'])\n",
    "\n",
    "# ‚ö†Ô∏è Remove entry_enc to avoid label leakage\n",
    "features = ['direction_enc', 'most_frequent_lane', 'duration', 'vehicle_type_enc']\n",
    "\n",
    "X = vehicle_data[features].values\n",
    "y = vehicle_data['exit_enc'].values\n",
    "\n",
    "# üé≤ Add Gaussian noise to simulate real-world imperfection\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 0.3, X.shape)\n",
    "X_noisy = X + noise\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_noisy)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models with grid search params\n",
    "models = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [5, 10]  # Reduce capacity\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [3, 5],  # Reduce complexity\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1],\n",
    "            \"solver\": [\"liblinear\", \"lbfgs\"]\n",
    "        }\n",
    "    },\n",
    "    \"MLP Classifier\": {\n",
    "        \"model\": MLPClassifier(max_iter=1000),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(64,), (128,)],\n",
    "            \"activation\": ['relu', 'tanh'],\n",
    "            \"alpha\": [0.001, 0.01]  # Increase regularization\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\nüîç Tuning and Training: {name}\")\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    preds = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    report = classification_report(y_test, preds, target_names=le_exit.classes_.astype(str))\n",
    "    \n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"classification_report\": report,\n",
    "        \"best_params\": clf.best_params_\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìà Model Comparison Results:\\n\")\n",
    "for model_name, res in results.items():\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"‚úÖ Accuracy: {res['accuracy']:.4f}\")\n",
    "    print(f\"üõ†Ô∏è Best Params: {res['best_params']}\")\n",
    "    print(f\"üìä Classification Report:\\n{res['classification_report']}\")\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab238772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
